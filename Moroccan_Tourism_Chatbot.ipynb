{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Moroccan_Tourism_Chatbot.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/BEKRINEY/Moroccan-Chatbot/blob/master/Moroccan_Tourism_Chatbot.ipynb",
      "authorship_tag": "ABX9TyMKGoDjQ4Kkf2oi0vpug5b6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BEKRINEY/Moroccan-Chatbot/blob/master/Moroccan_Tourism_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVc1RFzkjV3T",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "#**Moroccan Tourisme Chatbot**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPTbLnqAjnAg",
        "colab_type": "text"
      },
      "source": [
        "Un chatbot est un logiciel alimenté par l'intelligence artificielle d'un appareil (Siri, Alexa, Google Assistant, etc.), d'une application, d'un site web ou d'autres réseaux qui tentent d'évaluer les besoins des consommateurs et les aident ensuite à effectuer une tâche particulière, comme une transaction commerciale, une réservation d'hôtel, la soumission d'un formulaire, etc. Aujourd'hui, presque toutes les entreprises ont mis en place un chatbot pour dialoguer avec les utilisateurs. Voici quelques-unes des manières dont les entreprises utilisent les chatbots :\n",
        "\n",
        "- Pour fournir des informations sur les vols\n",
        "- pour relier les clients et leurs finances\n",
        "- En tant qu'assistance à la clientèle\n",
        "\n",
        "Les possibilités sont (presque) illimitées.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceCGkhOYkKCq",
        "colab_type": "text"
      },
      "source": [
        "**Morrocan Tourism Chatbot** est un chatbot qui permet d'aider les voyageurs qui vont visiter le maroc, alors il offre des informations sur la cuture , les places touristiques, les repas de notre cher pays.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mISvYSX6xtN",
        "colab_type": "text"
      },
      "source": [
        "**Ce projet est venu dans le contexte de la pandémie COVID-19 qui a impacté le domaine touristique dans le monde entier et surtout que notre pays a connu une baisse de plus de 20% des recettes du tourisme.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJCwZ7bcy2tt",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "![alt text](https://miro.medium.com/max/700/1*Jbbj376HkLMqqnIRAhsjKg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d99v78ZU8rZO",
        "colab_type": "text"
      },
      "source": [
        "#Comment les chatbots fonctionne ?\n",
        "\n",
        "Il existe en gros deux variantes de chatbots : Rule-Based et Self-learning..\n",
        "\n",
        "1- Dans une approche **Rule-Based**, un bot répond à des questions basées sur certaines règles sur lesquelles il est formé. Les règles définies peuvent être très simples à très complexes. Les bots peuvent traiter des questions simples mais ne peuvent pas gérer des questions complexes.\n",
        "\n",
        "2- Les bots **Self-learning** sont ceux qui utilisent certaines approches basées sur l'apprentissage automatique et sont définitivement plus efficaces que les bots basés sur des règles. Ces bots peuvent être de deux autres types : Retrieval Based ou bien Generative\n",
        "\n",
        "i) Dans les modèles **retrieval-based**, un chatbot utilise une certaine heuristique pour sélectionner une réponse à partir d'une bibliothèque de réponses prédéfinies. Le chatbot utilise le message et le contexte de la conversation pour sélectionner la meilleure réponse à partir d'une liste prédéfinie de messages de bot. Le contexte peut inclure une position actuelle dans l'arbre de dialogue, tous les messages précédents de la conversation, des variables préalablement enregistrées (par exemple, le nom d'utilisateur). Les heuristiques pour la sélection d'une réponse peuvent être conçues de différentes manières, de la logique conditionnelle \"if-else\" basée sur des règles aux classificateurs d'apprentissage machine.\n",
        "\n",
        "ii) Les bots générateurs (**Generative**) peuvent générer les réponses et pas toujours les réponses avec une des réponses d'un ensemble de réponses. Cela les rend plus intelligents car ils prennent mot à mot la requête et génèrent les réponses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWg_ku-ZzAuJ",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/700/1*4SzjHTccgX85iRrw589Y1g.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai1KiAviGerU",
        "colab_type": "text"
      },
      "source": [
        "#Les technologies utlisées  :\n",
        "\n",
        "-Scikit library et NLTK. \n",
        "\n",
        "-NLP (Natural Language Processing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Cn0O9dNHOaM",
        "colab_type": "text"
      },
      "source": [
        "#NLP (Natural Language Processing) :\n",
        "\n",
        "Le domaine d'étude qui se concentre sur les interactions entre le langage humain et les ordinateurs est appelé traitement du langage naturel, ou PNL en abrégé. Il se situe à l'intersection de l'informatique, de l'intelligence artificielle et de la linguistique informatique [Wikipedia]. La PNL est un moyen pour les ordinateurs pour analyser, comprendre et déduire le sens du langage humain d'une manière intelligente et **utile**. En utilisant la PNL, les développeurs peuvent organiser et structurer les connaissances pour effectuer des tâches telles que le résumé automatique, la traduction, la reconnaissance d'entités nommées, l'extraction de relations, l'analyse des sentiments, la reconnaissance de la parole et la segmentation de sujets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GczFqLkD3br",
        "colab_type": "text"
      },
      "source": [
        "#Téléchargement et installation de NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1kWv6yTHlIf",
        "colab_type": "text"
      },
      "source": [
        "NLTK (Natural Language Toolkit) est une plateforme de référence pour la réalisation des programmes Python destinés à fonctionner avec des données en langage humain. Elle fournit des interfaces faciles à utiliser pour plus de 50 corpus et ressources lexicales telles que WordNet, ainsi qu'une suite de bibliothèques de traitement de texte pour la classification, la tokenisation, le balisage, l'analyse et le raisonnement sémantique, des enveloppes pour les bibliothèques de PNL de niveau industriel.\n",
        "\n",
        "La NLTK a été qualifiée de \"merveilleux outil pour l'enseignement et le travail en linguistique informatique utilisant Python\" et de \"bibliothèque étonnante pour jouer avec le langage naturel\".\n",
        "\n",
        "Le traitement du langage naturel avec Python fournit une introduction pratique à la programmation pour le traitement du langage. Je recommande vivement ce livre aux personnes qui débutent en PNL avec Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CjScDd5Dhaa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "93d33f15-6a61-47f5-9669-a4b59cc61459"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDzIdASuS7jx",
        "colab_type": "text"
      },
      "source": [
        "#l'importation des bibliothèques nécessaires"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy1clcNDPOlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "import string # to process standard python strings\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXJHrJfGTeAp",
        "colab_type": "text"
      },
      "source": [
        "#Corpus\n",
        "\n",
        "Nous devons lire le fichier morocco.txt et convertir l'ensemble du corpus en une liste de phrases et une liste de mots pour un pré-traitement plus approfondi.\n",
        "\n",
        "Notre corpus Morocco.txt  contient plusieurs articles apropos du maroc , les villes , les repas marocaines .. récupérés depuis Wikipidia et les sites web de tourisme les plus connus via scraping "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOAg0xT2TodI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "4c65cfb1-b7ae-4f21-d454-5e80a461f25d"
      },
      "source": [
        "f=open('/content/drive/My Drive/Colab Notebooks/Morocco.txt','r',errors = 'ignore')\n",
        "raw=f.read()\n",
        "raw=raw.lower()# converts to lowercase\n",
        "nltk.download('punkt') # first-time use only\n",
        "nltk.download('wordnet') # first-time use only\n",
        "#Tokinization\n",
        "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
        "word_tokens = nltk.word_tokenize(raw)# converts to list of words\n",
        "nltk.download('popular', quiet=True) "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4VRgdluTZNb",
        "colab_type": "text"
      },
      "source": [
        "#Text Pre- Processing with NLTK\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eHukMfX8iPT",
        "colab_type": "text"
      },
      "source": [
        "Le principal problème avec les données textuelles est qu'elles sont toutes au format texte (strings). Cependant, les algorithmes d'apprentissage automatique ont besoin d'une sorte de vecteur de caractéristiques numériques pour effectuer la tâche. Ainsi, avant de commencer un projet de PNL, nous devons le pré-traiter pour le rendre idéal pour le travail. Le pré-traitement d'un texte de base comprend :\n",
        "\n",
        "- La conversion de l'ensemble du texte **en majuscules ou en minuscules**, afin que l'algorithme ne traite pas les mêmes mots dans différents cas comme des mots différents\n",
        "- **Tokenization :** La tokenisation est juste le terme utilisé pour décrire le processus de conversion des chaînes de texte normales en une liste de tokens, c'est-à-dire des mots que nous voulons vraiment. Le tokenizer de phrases peut être utilisé pour trouver la liste des phrases et le tokenizer de mots peut être utilisé pour trouver la liste des mots dans les chaînes.\n",
        "- **Removing Noise :** Supprimer le bruit, c'est-à-dire tout ce qui ne se trouve pas dans un numéro ou une lettre standard.\n",
        "-Removing Stop words : Suppression des mots d'arrêt. Parfois, certains mots très communs qui semblent peu utiles pour aider à sélectionner les documents correspondant à un besoin de l'utilisateur sont entièrement exclus du vocabulaire. Ces mots sont appelés \"mots d'arrêt\n",
        "-**Stemming:** Le \"stem\" est le processus qui consiste à réduire les mots infléchis (ou parfois dérivés) à leur tige, leur base ou leur racine - généralement une forme de mot écrit. Exemple : si nous devions trouver la racine des mots suivants :  “Stems”, “Stemming”, “Stemmed”, “and Stemtization”, le résultat serait un seul mot \"stem\".\n",
        "-**Lemmatization:** La lemmatisation est une légère variante du mot \"stemming \". La différence majeure entre les deux est que le stemming peut souvent créer des mots inexistants, alors que les lemmes sont de purs mots. Ainsi, votre racine, c'est-à-dire le mot avec lequel vous vous retrouvez, n'est pas quelque chose que vous pouvez simplement chercher dans un dictionnaire, mais vous pouvez chercher un lemme. Des exemples de lemmatisation sont que \"run\" est une forme de base pour des mots comme \"running\" ou \"ran\" ou que les mots \"better\" et \"good\" sont dans le même lemme, donc ils sont considérés comme identiques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJv_CUSTTxKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgPT4PVYT0cH",
        "colab_type": "text"
      },
      "source": [
        "#Keyword matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJY6DoA1g94K",
        "colab_type": "text"
      },
      "source": [
        "Ensuite, nous allons définir une fonction de bienvenue pour le bot, c'est-à-dire que si l'entrée d'un utilisateur est une salutation, le bot doit renvoyer une réponse de salutation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSh1EuFsT3SI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
        "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "def greeting(sentence):\n",
        " \n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUWUxbssT8lX",
        "colab_type": "text"
      },
      "source": [
        "#Generating Response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihs8XzqahtTa",
        "colab_type": "text"
      },
      "source": [
        "Afin de générer une réponse de notre bot pour les questions saisie par les utilisateurs, ,nous utiliserons le concept de similarité des documents. Nous avons donc commencé par importer les modules nécessaires.\n",
        "\n",
        "À partir de la bibliothèque Scikit Learn, importez TFidf vectorizer pour convertir une collection de documents bruts en une matrice de caractéristiques TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cLFTP49UhKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_MRL4UhaOlx",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY2hWuVJiclq",
        "colab_type": "text"
      },
      "source": [
        "Importer aussi lemodule de la similarité des cosinus de la bibliothèque Scikit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90Hnh3_YUm2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtatHiJNihR4",
        "colab_type": "text"
      },
      "source": [
        "Il sera utilisé pour trouver la similarité entre les mots saisis par l'utilisateur et les mots du corpus. \n",
        "\n",
        "Nous définissons une fonction de réponse qui recherche dans l'énoncé de l'utilisateur un ou plusieurs mots-clés connus et qui renvoie une réponse parmi plusieurs possibles. S'il ne trouve pas l'entrée correspondant à l'un des mots clés, il renvoie une réponse : \" Je suis désolé ! Je ne vous comprends pas\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGjjI-BlUvHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def response(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)   \n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2] \n",
        "    if(req_tfidf==0):\n",
        "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYeZlghsmv7F",
        "colab_type": "text"
      },
      "source": [
        "Enfin, nous allons préparer les phrases que nous souhaitons notre robot dise au début et en fin des conversations en fonction de la saisie de l'utilisateur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCORsg7hUx1R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "76b39627-fdb5-42f1-e538-c854f5c9ae79"
      },
      "source": [
        "flag=True\n",
        "print(\"MOROBOT: My name is MOROBOT. I will answer your queries about Morocco. If you want to exit, type Bye!\")\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response!='bye'):\n",
        "        if(user_response=='thanks ' or user_response=='thank you' ):\n",
        "            flag=False\n",
        "            print(\"MOROBO: You are welcome..\")\n",
        "        else:\n",
        "            if(greeting(user_response)!=None):\n",
        "                print(\"MOROBO: \"+greeting(user_response))\n",
        "            else:\n",
        "                print(\"MOROBO: \",end=\"\")\n",
        "                print(response(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"MOROBO: Bye! take care..\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MOROBOT: My name is MOROBOT. I will answer your queries about Morocco. If you want to exit, type Bye!\n",
            "hi\n",
            "MOROBO: hello\n",
            "tell me sothing about morocco\n",
            "MOROBO: "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "they are colorful and tell stories and traditions of different amazigh tribes in morocco, accompanied by the sounds of flutes, tambourines, and rhythmic hand clapping.\n",
            "what is couscous\n",
            "MOROBO: "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[191] \n",
            "\n",
            "\n",
            "couscous           \n",
            "\n",
            "couscous is a purely moroccan dish.\n",
            "tell me about marrakech\n",
            "MOROBO: "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "in 2001, the first international film festival of marrakech (fifm) was also held in marrakech.\n",
            "bye\n",
            "MOROBO: Bye! take care..\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
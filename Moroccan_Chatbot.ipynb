{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Moroccan-Chatbot",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/BEKRINEY/Moroccan-Chatbot/blob/master/Moroccan_Chatbot.ipynb",
      "authorship_tag": "ABX9TyOYhctjgETipFH4iphM4E75",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BEKRINEY/Moroccan-Chatbot/blob/master/Moroccan_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5l9KAQ_35ix",
        "colab_type": "text"
      },
      "source": [
        "#Introduction : "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjjS28m83ssP",
        "colab_type": "text"
      },
      "source": [
        "A chatbot is an artificial intelligence-powered piece of software in a device (Siri, Alexa, Google Assistant etc), application, website or other networks that try to gauge consumer’s needs and then assist them to perform a particular task like a commercial transaction, hotel booking, form submission etc . Today almost every company has a chatbot deployed to engage with the users. By 2025, chatbots will be handling 80 percent of customer-service interactions; they are already handling about 60 percent of transactions now in differents languages. In our country morocco, all companies are using chatbots with Arabic or French language, but not all Moroccans speak those languages and the majority of citizens prefers to communicate with dialect Moroccan Arabic language. So that's inspired me to make a Moroccan chatbot who can engage with users with dialect Moroccan Arabic , using cikit library and NLTK."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VtZqyk8gLZz",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#Import necessary libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCsZYIz4DB73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import random\n",
        "import string # to process standard python strings\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65notkFPDZiY",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuixpA9-o6Yu",
        "colab_type": "text"
      },
      "source": [
        "#Downloading and installing NLTK "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO66eheygiKX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ac5aaf57-3516-4483-977a-2326790b3267"
      },
      "source": [
        "pip install nltk\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34zbxeW41Tr6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "b19175e8-0f0b-4e5c-9037-66cb79921a3a"
      },
      "source": [
        "pip install rake-nltk\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rake-nltk\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/c4/b4ff57e541ac5624ad4b20b89c2bafd4e98f29fd83139f3a81858bdb3815/rake_nltk-1.0.4.tar.gz\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rake-nltk) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->rake-nltk) (1.12.0)\n",
            "Building wheels for collected packages: rake-nltk\n",
            "  Building wheel for rake-nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rake-nltk: filename=rake_nltk-1.0.4-py2.py3-none-any.whl size=7819 sha256=c2ad6b873bce342067803095a7138af1c32b484199778a29fc22eb5b34f2600e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/92/fc/271b3709e71a96ffe934b27818946b795ac6b9b8ff8682483f\n",
            "Successfully built rake-nltk\n",
            "Installing collected packages: rake-nltk\n",
            "Successfully installed rake-nltk-1.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehliBZxx1Zft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "005dcd1e-0ea0-4417-de4c-da274920e5a0"
      },
      "source": [
        "pip install polyglot"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting polyglot\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/98/e24e2489114c5112b083714277204d92d372f5bbe00d5507acf40370edb9/polyglot-16.7.4.tar.gz (126kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: polyglot\n",
            "  Building wheel for polyglot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for polyglot: filename=polyglot-16.7.4-py2.py3-none-any.whl size=52557 sha256=3e207f221e46aa33234ffcddb4b6cc4d57d9554dc0fcb533478991a52b509bf0\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/91/ef/f1369fdc1203b0a9347d4b24f149b83a305f39ab047986d9da\n",
            "Successfully built polyglot\n",
            "Installing collected packages: polyglot\n",
            "Successfully installed polyglot-16.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNQt4DhagPwJ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGwNrxqypIcm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Installing NLTK Packages¶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUo231FgDZ52",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "53051d1f-2bc7-499e-e176-56bc5acef830"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('popular', quiet=True) # for downloading packages\n",
        "#nltk.download('punkt') # first-time use only\n",
        "#nltk.download('wordnet') # first-time use only"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwtYPtGmpRdj",
        "colab_type": "text"
      },
      "source": [
        "#Reading in the corpus "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "567FmVSCq8EP",
        "colab_type": "text"
      },
      "source": [
        "Using Corpus file from MADAR Project  :\n",
        "\n",
        " The MADAR corpus is a collection of parallel sentences covering the dialects of 25 cities from the Arab World, in addition to English, French, and MSA. The corpus is created by translating selected sentences from the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2007) to the different dialects. The exact details on the translation process and source and target languages are described in Bouamor et al. (2018).\n",
        "\n",
        "The list of Arab cities covered in the MADAR corpus includes: Aleppo, Alexandria, Algiers, Amman, Aswan, Baghdad, Basra, Beirut, Benghazi, Cairo, Damascus, Doha, Fes, Jeddah, Jerusalem, Khartoum, Mosul, Muscat, Rabat, Riyadh, Salt, Sanaa, Sfax, Tripoli, and Tunis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlqNowDDcx1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f=open('/content/drive/My Drive/Colab Notebooks/MADAR.corpus6.Rabat','r',errors = 'ignore')\n",
        "raw=f.read()\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAr9pXnqppQ7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#Tokenisation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvQOY_lGc5wZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
        "word_tokens = nltk.word_tokenize(raw)# converts to list of words"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrnUN3_epyDL",
        "colab_type": "text"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XorHKbwc6w_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK5-RAYXp7sY",
        "colab_type": "text"
      },
      "source": [
        "#Keyword matching "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kiJZL3_c88_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GREETING_INPUTS = (\"السلام\", \"اهلا\", \"صباح الخير\", \"مساء النور\", \"السلام عليكم\",\"مرحبا\",)\n",
        "GREETING_RESPONSES = [\"وعليكم و  السلام\", \"اهلا\", \"*الحمد لله*\", \"صباح النور\", \"مرحبا بك\", \" كايشرفني نتكلم معاك !\"]\n",
        "def greeting(sentence):\n",
        " \n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZQycBr7qAw5",
        "colab_type": "text"
      },
      "source": [
        "#Generating Response "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMWNqN3odBdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def response(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='arabic')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0):\n",
        "        robo_response=robo_response+\"سمح ليا مافمتش ؟ ممكن تعاود بطريقةاخرى باش نفهم\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z0nRBwoxgIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "c0ecfa0f-dd6f-4734-ce85-54325783871d"
      },
      "source": [
        "flag=True\n",
        "print(\"موربوت: سميتي موروبوت , سولني فاي حاجة جات فبالك و غادي نحاول نجاوبك!\")\n",
        "\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response!='بسلامة'):\n",
        "        if(user_response=='شكرا' or user_response=='شكرا جزيلا' ):\n",
        "            flag=False\n",
        "            print(\"العفو صديقي .. مرحبا في اي وقت\")\n",
        "        else:\n",
        "            if(greeting(user_response)!=None):\n",
        "                print(\"موروبوت :\"+greeting(user_response))\n",
        "            else:\n",
        "                print(\"موروبوت: \",end=\"\")\n",
        "                print(response(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"موروبوت: بسلامة ! طهلا فراسك صديقي..\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "موربوت: سميتي موروبوت , سولني فاي حاجة جات فبالك و غادي نحاول نجاوبك!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
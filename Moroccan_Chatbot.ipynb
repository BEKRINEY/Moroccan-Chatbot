{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Moroccan-Chatbot",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/BEKRINEY/Moroccan-Chatbot/blob/master/Moroccan_Chatbot.ipynb",
      "authorship_tag": "ABX9TyO5j+GH2g3VGHY28wOCl8G2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BEKRINEY/Moroccan-Chatbot/blob/master/Moroccan_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5l9KAQ_35ix",
        "colab_type": "text"
      },
      "source": [
        "#Introduction : "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjjS28m83ssP",
        "colab_type": "text"
      },
      "source": [
        "A chatbot is an artificial intelligence-powered piece of software in a device (Siri, Alexa, Google Assistant etc), application, website or other networks that try to gauge consumer’s needs and then assist them to perform a particular task like a commercial transaction, hotel booking, form submission etc . Today almost every company has a chatbot deployed to engage with the users. By 2025, chatbots will be handling 80 percent of customer-service interactions; they are already handling about 60 percent of transactions now in differents languages. \n",
        "\n",
        "**In our country morocco, all companies are using chatbots with Arabic or French language, but not all Moroccans speak those languages and the majority of citizens prefers to communicate with dialect Moroccan Arabic language. So that's inspired me to make a Moroccan chatbot who can engage with users with dialect Moroccan Arabic , using cikit library and NLTK.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox-ipNDfXQ-3",
        "colab_type": "text"
      },
      "source": [
        "#Comment les chatbots fonctionne ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szuln6gTXWZp",
        "colab_type": "text"
      },
      "source": [
        "Il existe en gros deux variantes de chatbots : Rule-Based et Self-learning..\n",
        "\n",
        "   1- Dans une approche **Rule-Based**, un bot répond à des questions basées sur certaines règles sur lesquelles il est formé. Les règles définies peuvent être très simples à très complexes. Les bots peuvent traiter des questions simples mais ne peuvent pas gérer des questions complexes.\n",
        "   2- Les bots **Self-learning** sont ceux qui utilisent certaines approches basées sur l'apprentissage automatique et sont définitivement plus efficaces que les bots basés sur des règles. Ces bots peuvent être de deux autres types : Retrieval Based ou bien Generative\n",
        "\n",
        "i) Dans les modèles **retrieval-based**, un chatbot utilise une certaine heuristique pour sélectionner une réponse à partir d'une bibliothèque de réponses prédéfinies. Le chatbot utilise le message et le contexte de la conversation pour sélectionner la meilleure réponse à partir d'une liste prédéfinie de messages de bot. Le contexte peut inclure une position actuelle dans l'arbre de dialogue, tous les messages précédents de la conversation, des variables préalablement enregistrées (par exemple, le nom d'utilisateur). Les heuristiques pour la sélection d'une réponse peuvent être conçues de différentes manières, de la logique conditionnelle \"if-else\" basée sur des règles aux classificateurs d'apprentissage machine.\n",
        "\n",
        "ii) Les bots générateurs (**Generative**) peuvent générer les réponses et pas toujours les réponses avec une des réponses d'un ensemble de réponses. Cela les rend plus intelligents car ils prennent mot à mot la requête et génèrent les réponses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFvQZ0B_XbtD",
        "colab_type": "text"
      },
      "source": [
        "#Les technologies utlisées  :\n",
        "\n",
        "-Scikit library et NLTK. \n",
        "\n",
        "-NLP (Natural Language Processing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoTNgK_SXi1a",
        "colab_type": "text"
      },
      "source": [
        "#NLP (Natural Language Processing) :\n",
        "\n",
        "Le domaine d'étude qui se concentre sur les interactions entre le langage humain et les ordinateurs est appelé traitement du langage naturel, ou PNL en abrégé. Il se situe à l'intersection de l'informatique, de l'intelligence artificielle et de la linguistique informatique [Wikipedia]. La PNL est un moyen pour les ordinateurs pour analyser, comprendre et déduire le sens du langage humain d'une manière intelligente et **utile**. En utilisant la PNL, les développeurs peuvent organiser et structurer les connaissances pour effectuer des tâches telles que le résumé automatique, la traduction, la reconnaissance d'entités nommées, l'extraction de relations, l'analyse des sentiments, la reconnaissance de la parole et la segmentation de sujets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzK3XONcXrpZ",
        "colab_type": "text"
      },
      "source": [
        "# Téléchargement et installation de NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJWXhypfX1u4",
        "colab_type": "text"
      },
      "source": [
        "NLTK (Natural Language Toolkit) est une plateforme de référence pour la réalisation des programmes Python destinés à fonctionner avec des données en langage humain. Elle fournit des interfaces faciles à utiliser pour plus de 50 corpus et ressources lexicales telles que WordNet, ainsi qu'une suite de bibliothèques de traitement de texte pour la classification, la tokenisation, le balisage, l'analyse et le raisonnement sémantique, des enveloppes pour les bibliothèques de PNL de niveau industriel.\n",
        "\n",
        "La NLTK a été qualifiée de \"merveilleux outil pour l'enseignement et le travail en linguistique informatique utilisant Python\" et de \"bibliothèque étonnante pour jouer avec le langage naturel\".\n",
        "\n",
        "Le traitement du langage naturel avec Python fournit une introduction pratique à la programmation pour le traitement du langage. Je recommande vivement ce livre aux personnes qui débutent en PNL avec Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oblEb-DfXt8e",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UcEC9WjX-pE",
        "colab_type": "text"
      },
      "source": [
        "**Installer NLTK via la commande :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woWRKbBVYA3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VtZqyk8gLZz",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#l'importation des bibliothèques nécessaires\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCsZYIz4DB73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import random\n",
        "import string # to process standard python strings\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65notkFPDZiY",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34zbxeW41Tr6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "b19175e8-0f0b-4e5c-9037-66cb79921a3a"
      },
      "source": [
        "pip install rake-nltk\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rake-nltk\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/c4/b4ff57e541ac5624ad4b20b89c2bafd4e98f29fd83139f3a81858bdb3815/rake_nltk-1.0.4.tar.gz\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rake-nltk) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->rake-nltk) (1.12.0)\n",
            "Building wheels for collected packages: rake-nltk\n",
            "  Building wheel for rake-nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rake-nltk: filename=rake_nltk-1.0.4-py2.py3-none-any.whl size=7819 sha256=c2ad6b873bce342067803095a7138af1c32b484199778a29fc22eb5b34f2600e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/92/fc/271b3709e71a96ffe934b27818946b795ac6b9b8ff8682483f\n",
            "Successfully built rake-nltk\n",
            "Installing collected packages: rake-nltk\n",
            "Successfully installed rake-nltk-1.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehliBZxx1Zft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "005dcd1e-0ea0-4417-de4c-da274920e5a0"
      },
      "source": [
        "pip install polyglot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting polyglot\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/98/e24e2489114c5112b083714277204d92d372f5bbe00d5507acf40370edb9/polyglot-16.7.4.tar.gz (126kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: polyglot\n",
            "  Building wheel for polyglot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for polyglot: filename=polyglot-16.7.4-py2.py3-none-any.whl size=52557 sha256=3e207f221e46aa33234ffcddb4b6cc4d57d9554dc0fcb533478991a52b509bf0\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/91/ef/f1369fdc1203b0a9347d4b24f149b83a305f39ab047986d9da\n",
            "Successfully built polyglot\n",
            "Installing collected packages: polyglot\n",
            "Successfully installed polyglot-16.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNQt4DhagPwJ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGwNrxqypIcm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Installer NLTK Packages :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUo231FgDZ52",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "53051d1f-2bc7-499e-e176-56bc5acef830"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('popular', quiet=True) # for downloading packages\n",
        "#nltk.download('punkt') # first-time use only\n",
        "#nltk.download('wordnet') # first-time use only"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwtYPtGmpRdj",
        "colab_type": "text"
      },
      "source": [
        "#Reading in the corpus "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "567FmVSCq8EP",
        "colab_type": "text"
      },
      "source": [
        "Using Corpus file from MADAR Project  :\n",
        "\n",
        " The MADAR corpus is a collection of parallel sentences covering the dialects of 25 cities from the Arab World, in addition to English, French, and MSA. The corpus is created by translating selected sentences from the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2007) to the different dialects. The exact details on the translation process and source and target languages are described in Bouamor et al. (2018).\n",
        "\n",
        "The list of Arab cities covered in the MADAR corpus includes: Aleppo, Alexandria, Algiers, Amman, Aswan, Baghdad, Basra, Beirut, Benghazi, Cairo, Damascus, Doha, Fes, Jeddah, Jerusalem, Khartoum, Mosul, Muscat, Rabat, Riyadh, Salt, Sanaa, Sfax, Tripoli, and Tunis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlqNowDDcx1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f=open('/content/drive/My Drive/Colab Notebooks/MADAR.corpus6.Rabat','r',errors = 'ignore')\n",
        "raw=f.read()\n",
        "nltk.download('punkt') # first-time use only\n",
        "nltk.download('wordnet') # first-time use only\n",
        "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
        "word_tokens = nltk.word_tokenize(raw)# converts to list of words\n",
        "nltk.download('popular', quiet=True) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrnUN3_epyDL",
        "colab_type": "text"
      },
      "source": [
        "#Text Pre- Processing with NLTK\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFZvwfdQZ54c",
        "colab_type": "text"
      },
      "source": [
        "Le principal problème avec les données textuelles est qu'elles sont toutes au format texte (strings). Cependant, les algorithmes d'apprentissage automatique ont besoin d'une sorte de vecteur de caractéristiques numériques pour effectuer la tâche. Ainsi, avant de commencer un projet de PNL, nous devons le pré-traiter pour le rendre idéal pour le travail. Le pré-traitement d'un texte de base comprend :\n",
        "\n",
        "- La conversion de l'ensemble du texte **en majuscules ou en minuscules**, afin que l'algorithme ne traite pas les mêmes mots dans différents cas comme des mots différents\n",
        "- **Tokenization :** La tokenisation est juste le terme utilisé pour décrire le processus de conversion des chaînes de texte normales en une liste de tokens, c'est-à-dire des mots que nous voulons vraiment. Le tokenizer de phrases peut être utilisé pour trouver la liste des phrases et le tokenizer de mots peut être utilisé pour trouver la liste des mots dans les chaînes.\n",
        "- **Removing Noise :** Supprimer le bruit, c'est-à-dire tout ce qui ne se trouve pas dans un numéro ou une lettre standard.\n",
        "-Removing Stop words : Suppression des mots d'arrêt. Parfois, certains mots très communs qui semblent peu utiles pour aider à sélectionner les documents correspondant à un besoin de l'utilisateur sont entièrement exclus du vocabulaire. Ces mots sont appelés \"mots d'arrêt\n",
        "-**Stemming:** Le \"stem\" est le processus qui consiste à réduire les mots infléchis (ou parfois dérivés) à leur tige, leur base ou leur racine - généralement une forme de mot écrit. Exemple : si nous devions trouver la racine des mots suivants :  “Stems”, “Stemming”, “Stemmed”, “and Stemtization”, le résultat serait un seul mot \"stem\".\n",
        "-**Lemmatization:** La lemmatisation est une légère variante du mot \"stemming \". La différence majeure entre les deux est que le stemming peut souvent créer des mots inexistants, alors que les lemmes sont de purs mots. Ainsi, votre racine, c'est-à-dire le mot avec lequel vous vous retrouvez, n'est pas quelque chose que vous pouvez simplement chercher dans un dictionnaire, mais vous pouvez chercher un lemme. Des exemples de lemmatisation sont que \"run\" est une forme de base pour des mots comme \"running\" ou \"ran\" ou que les mots \"better\" et \"good\" sont dans le même lemme, donc ils sont considérés comme identiques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XorHKbwc6w_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK5-RAYXp7sY",
        "colab_type": "text"
      },
      "source": [
        "#Keyword matching "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmCWXfKwaMXJ",
        "colab_type": "text"
      },
      "source": [
        "Ensuite, nous allons définir une fonction de bienvenue pour le bot, c'est-à-dire que si l'entrée d'un utilisateur est une salutation, le bot doit renvoyer une réponse de salutation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kiJZL3_c88_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GREETING_INPUTS = (\"السلام\", \"اهلا\", \"صباح الخير\", \"مساء النور\", \"السلام عليكم\",\"مرحبا\",)\n",
        "GREETING_RESPONSES = [\"وعليكم و  السلام\", \"اهلا\", \"*الحمد لله*\", \"صباح النور\", \"مرحبا بك\", \" كايشرفني نتكلم معاك !\"]\n",
        "def greeting(sentence):\n",
        " \n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZQycBr7qAw5",
        "colab_type": "text"
      },
      "source": [
        "#Generating Response "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN02EPLgaN7H",
        "colab_type": "text"
      },
      "source": [
        "Afin de générer une réponse de notre bot pour les questions saisie par les utilisateurs, ,nous utiliserons le concept de similarité des documents. Nous avons donc commencé par importer les modules nécessaires.\n",
        "\n",
        "À partir de la bibliothèque Scikit Learn, importez TFidf vectorizer pour convertir une collection de documents bruts en une matrice de caractéristiques TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HyY667yaj5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZbh83K_aXR9",
        "colab_type": "text"
      },
      "source": [
        "Importer aussi lemodule de la similarité des cosinus de la bibliothèque Scikit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUONygrBanHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWc2xjeuags_",
        "colab_type": "text"
      },
      "source": [
        "Il sera utilisé pour trouver la similarité entre les mots saisis par l'utilisateur et les mots du corpus.\n",
        "\n",
        "Nous définissons une fonction de réponse qui recherche dans l'énoncé de l'utilisateur un ou plusieurs mots-clés connus et qui renvoie une réponse parmi plusieurs possibles. S'il ne trouve pas l'entrée correspondant à l'un des mots clés, il renvoie une réponse : \" Je suis désolé ! Je ne vous comprends pas\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMWNqN3odBdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def response(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='arabic')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0):\n",
        "        robo_response=robo_response+\"سمح ليا مافمتش ؟ ممكن تعاود بطريقةاخرى باش نفهم\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBILLcb5arbZ",
        "colab_type": "text"
      },
      "source": [
        "Enfin, nous allons préparer les phrases que nous souhaitons notre robot dise au début et en fin des conversations en fonction de la saisie de l'utilisateur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z0nRBwoxgIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "c0ecfa0f-dd6f-4734-ce85-54325783871d"
      },
      "source": [
        "flag=True\n",
        "print(\"موربوت: سميتي موروبوت , سولني فاي حاجة جات فبالك و غادي نحاول نجاوبك!\")\n",
        "\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response!='بسلامة'):\n",
        "        if(user_response=='شكرا' or user_response=='شكرا جزيلا' ):\n",
        "            flag=False\n",
        "            print(\"العفو صديقي .. مرحبا في اي وقت\")\n",
        "        else:\n",
        "            if(greeting(user_response)!=None):\n",
        "                print(\"موروبوت :\"+greeting(user_response))\n",
        "            else:\n",
        "                print(\"موروبوت: \",end=\"\")\n",
        "                print(response(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"موروبوت: بسلامة ! طهلا فراسك صديقي..\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "موربوت: سميتي موروبوت , سولني فاي حاجة جات فبالك و غادي نحاول نجاوبك!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}